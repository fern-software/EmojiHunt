{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9fEx75gF9Sq"
      },
      "source": [
        "# Emoji Hunt Challenge - ML Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82AYpZcsF0vR"
      },
      "source": [
        "## Google collab pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLQy_OBtFx3W",
        "outputId": "61b39c83-b64e-4cae-97ea-12b41e446cd2"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb\n",
        "!pip install emojihunt --upgrade --q\n",
        "print(\"Current project version:\")\n",
        "!pip freeze | grep emojihunt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPEQ60xXGfFN",
        "outputId": "6e8f7cee-e957-48e5-9dcd-4c3cd6c8384c"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    model_file = '/content/gdrive/MyDrive/emoji_hunt_model.h5'\n",
        "except ImportError:\n",
        "    model_file = 'emoji_hunt_model.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saiw_2UwGUhR"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q8-KEgwTBKe",
        "outputId": "70c9c039-bed2-406c-e64a-5680ea768709"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from emojihunt import *\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import callbacks\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Activation, LeakyReLU, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, GlobalAveragePooling2D, Concatenate, UpSampling2D, Conv2DTranspose\n",
        "from keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.utils import Sequence\n",
        "import json\n",
        "import os\n",
        "from skimage.segmentation import slic, mark_boundaries, clear_border, find_boundaries\n",
        "from skimage.morphology import binary_dilation\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.measure import label, regionprops\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"Num GPUs:\", len(gpu_devices))\n",
        "for device in gpu_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8yI5Y1MEykl"
      },
      "source": [
        "Basic hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldjMXvxqEykt"
      },
      "outputs": [],
      "source": [
        "lr = 0.0001\n",
        "downsample_f = 2\n",
        "img_width = img_height = 512 // downsample_f\n",
        "batch_size = downsample_f * 4\n",
        "\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnpnn5PSEyk0"
      },
      "source": [
        "Create our own emojihunt object that returns a segmentation mask with each image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaQlGYtEEyk5"
      },
      "outputs": [],
      "source": [
        "class SegEmojiHunt(EmojiHunt):\n",
        "    def generate_image_and_points(self):\n",
        "        '''\n",
        "        Returns a target image, emoji example (uncorrupted), and ground truth gt_points. For your testing and development\n",
        "        '''\n",
        "        # Seed the random number generator to be very random\n",
        "        random.seed()\n",
        "        emoji_target = self.get_random_emoji_img()\n",
        "        test_image   = self.get_background()\n",
        "\n",
        "        gt_points = []\n",
        "        mask = np.zeros(test_image.shape[:-1], dtype=np.uint8)\n",
        "        tmp_mask = np.zeros_like(mask)\n",
        "        for _ in range(random.randint(1,10)):\n",
        "            x = random.randint(0,512-self.emoji_size-1)\n",
        "            y = random.randint(0,512-self.emoji_size-1)\n",
        "            gt_points.append((x+self.emoji_size//downsample_f,y+self.emoji_size//downsample_f))\n",
        "            augmented_emoji = self.augment_emoji(emoji_target)\n",
        "            emoji_mask = np.sum(augmented_emoji,axis=-1) > 25\n",
        "            \n",
        "            # Compare the intersection of the new mask and the previous mask. If there's any overlap then we trace a line of zeros around the edge of the intersection\n",
        "            tmp_mask[:,:] = 0\n",
        "            tmp_mask[x:x+self.emoji_size,y:y+self.emoji_size] = emoji_mask\n",
        "            intersection = np.logical_and(tmp_mask, mask)\n",
        "            if np.any(intersection):\n",
        "                mask[x:x+self.emoji_size,y:y+self.emoji_size] = np.where(\n",
        "                    np.sum(augmented_emoji,axis=-1) > 25, emoji_mask, mask[x:x+self.emoji_size,y:y+self.emoji_size])\n",
        "   \n",
        "                # Find the contour of the intersection\n",
        "                contour = find_boundaries(tmp_mask, mode='thick')\n",
        "                \n",
        "                # Trace a line of zeros around the contour\n",
        "                mask[contour] = 0\n",
        "\n",
        "            else:\n",
        "                mask[x:x+self.emoji_size,y:y+self.emoji_size] = np.where(\n",
        "                    np.sum(augmented_emoji,axis=-1) > 25, emoji_mask, mask[x:x+self.emoji_size,y:y+self.emoji_size])\n",
        "            \n",
        "            test_image[x:x+self.emoji_size,y:y+self.emoji_size] = np.where(\n",
        "                np.expand_dims(np.sum(augmented_emoji,axis=-1) > 25,-1), augmented_emoji, test_image[x:x+self.emoji_size,y:y+self.emoji_size])\n",
        "        \n",
        "        test_image_agus = [iaa.AdditiveGaussianNoise(scale=(0, 0.1*255)),\n",
        "                            iaa.GaussianBlur(sigma=(0.0, 0.25))]\n",
        "\n",
        "        test_image = iaa.Sequential(test_image_agus)(image=test_image)\n",
        "\n",
        "        return test_image, emoji_target, gt_points, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdsd7W5wEylB"
      },
      "source": [
        "Data Generator to feed the model. Resizes the input image, converts it to LAB, and normalizes it to [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KkAp-M7EylF",
        "outputId": "cc2747e0-f942-4f7e-be0e-43e1d9d6aea8"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    emoji_hunt_object = SegEmojiHunt()\n",
        "\n",
        "    def __init__(self, batch_size, augment, num_batches=100):\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.num_batches = num_batches\n",
        "        self.datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.xception.preprocess_input)\n",
        "\n",
        "    def __len__(self):\n",
        "        return batch_size*self.num_batches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        'Generate one batch of data'\n",
        "        X = np.empty((self.batch_size, *input_shape), dtype=np.uint8)\n",
        "        Y = np.zeros((self.batch_size, img_width, img_height), dtype=np.uint8)\n",
        "        C = np.zeros_like(Y)\n",
        "        for i in range(self.batch_size):\n",
        "            target_image, emoji, ground_truth, mask = self.emoji_hunt_object.generate_image_and_points()\n",
        "            # Resize\n",
        "            target_image = cv2.resize(target_image, (img_width, img_height), interpolation=cv2.INTER_AREA)\n",
        "            mask = cv2.resize(mask.astype('uint8'), (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            X[i,] = target_image\n",
        "            Y[i] = mask\n",
        "\n",
        "        # Normalize\n",
        "        self.datagen.standardize(X)\n",
        "        assert img_width == img_height\n",
        "        \n",
        "        return X, {'segmentation': Y}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdaRyDREylK"
      },
      "source": [
        "Generate some example masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k4qyVjkTEylN",
        "outputId": "b9ef3cdf-5f37-4e2d-9e1a-3403b856f5a1"
      },
      "outputs": [],
      "source": [
        "train_ds = DataGenerator(batch_size, augment=False, num_batches=1200//batch_size)\n",
        "val_ds = DataGenerator(batch_size, augment=False, num_batches=20) # Only used for scoring, generates the same data as train but with a smaller batch size\n",
        "\n",
        "batch = train_ds[0]\n",
        "for i in range(3):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    # show color scale\n",
        "    ax = plt.subplot(1, 2, 1)\n",
        "    # plt.imshow(cv2.cvtColor(batch[0][i], cv2.COLOR_LAB2RGB))\n",
        "    plt.imshow(cv2.cvtColor(batch[0][i], cv2.COLOR_BGR2RGB))\n",
        "    plt.subplot(1, 2, 2, sharex=ax, sharey=ax)\n",
        "    plt.imshow(batch[1]['segmentation'][i])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdDljJ-9EylT"
      },
      "source": [
        "Setup for weights and biases logging (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHJja8-OEylW"
      },
      "outputs": [],
      "source": [
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"emoji-hunt\",\n",
        "\n",
        "#     # track hyperparameters and run metadata with wandb.config\n",
        "#     config={\n",
        "#         \"batch_size\": batch_size\n",
        "#     }\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWl6HbEzEylb"
      },
      "source": [
        "https://pyimagesearch.com/2022/02/21/u-net-image-segmentation-in-keras/ for the U-Net architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsDa_-_4Eyle"
      },
      "outputs": [],
      "source": [
        "def double_conv_block(x, n_filters):\n",
        "   x = Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
        "   x = Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
        "   return x\n",
        "\n",
        "def downsample_block(x, n_filters):\n",
        "   f = double_conv_block(x, n_filters)\n",
        "   p = MaxPooling2D()(f)\n",
        "   p = Dropout(0.3)(p)\n",
        "   return f, p\n",
        "\n",
        "def upsample_block(x, conv_features, n_filters):\n",
        "   x = Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
        "   x = Concatenate(axis=-1)([x, conv_features])\n",
        "   x = Dropout(0.3)(x)\n",
        "   x = double_conv_block(x, n_filters)\n",
        "   return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D89nLe8oEylf",
        "outputId": "28c23e48-bd51-4ccf-9518-6249e96e30ad"
      },
      "outputs": [],
      "source": [
        "# From https://stackoverflow.com/questions/68481772/why-my-iou-keep-decrease-in-training-with-tensorflow-keras\n",
        "def iou(y_true, y_pred):\n",
        "    y_true = keras.backend.flatten(y_true)\n",
        "    y_pred = keras.backend.flatten(tf.argmax(y_pred, -1))\n",
        "    y_true_f = tf.cast(y_true, tf.float32)\n",
        "    y_pred_f = tf.cast(y_pred, tf.float32)\n",
        "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
        "    union = keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) - intersection\n",
        "    return (intersection + 1e-7) / (union + 1e-7)\n",
        "\n",
        "def iou_loss(y_true, y_pred):\n",
        "        return 1.0 - iou(y_true, y_pred)\n",
        "\n",
        "if os.path.exists(model_file):\n",
        "    print(f'Loading model from file: {model_file}')\n",
        "    model = load_model(model_file, custom_objects={'iou': iou})\n",
        "else:\n",
        "    in1 = keras.Input(shape=input_shape)\n",
        "\n",
        "    # encoder: contracting path - downsample\n",
        "    f1, p1 = downsample_block(in1, 64)\n",
        "    f2, p2 = downsample_block(p1, 128)\n",
        "    f3, p3 = downsample_block(p2, 256)\n",
        "    f4, p4 = downsample_block(p3, 512)\n",
        "    # Bottleneck\n",
        "    bottleneck = double_conv_block(p4, 1024)\n",
        "    # decoder: expanding path - upsample\n",
        "    u6 = upsample_block(bottleneck, f4, 512)\n",
        "    u7 = upsample_block(u6, f3, 256)\n",
        "    u8 = upsample_block(u7, f2, 128)\n",
        "    u9 = upsample_block(u8, f1, 64)\n",
        "    # outputs\n",
        "    output_mask = Conv2D(2, 1, padding=\"same\", activation=\"softmax\", name='segmentation')(u9)\n",
        "\n",
        "    model = keras.Model(in1, {\"segmentation\": output_mask}, name=\"U-Net\")\n",
        "\n",
        "    model.compile(loss={\"segmentation\": 'sparse_categorical_crossentropy'},\n",
        "                optimizer=Adam(learning_rate=lr),\n",
        "                metrics={\"segmentation\": ['accuracy', iou]})\n",
        "    \n",
        "    keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True, to_file='model.png', show_layer_names=False)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHOvzA5TEylh"
      },
      "outputs": [],
      "source": [
        "# early_stopping = callbacks.EarlyStopping(monitor='iou', mode='max', patience=4, restore_best_weights=True, start_from_epoch=5)\n",
        "# tensorboard = callbacks.TensorBoard(log_dir=f'./logs/run{np.random.randint(1,1000)}', histogram_freq=0, write_graph=True, write_images=True, embeddings_freq=5)\n",
        "# reduce_lr = callbacks.ReduceLROnPlateau(monitor='iou', mode='max', factor=0.2, patience=1, min_lr=0.000001, min_delta=0.0003)\n",
        "# checkpoint = callbacks.ModelCheckpoint(f'./model.h5', monitor='iou', mode='max', verbose=0, save_best_only=True, save_weights_only=False, save_freq='epoch')\n",
        "# model.fit(train_ds,\n",
        "#         batch_size=batch_size,\n",
        "#         epochs=70,\n",
        "#         callbacks=[early_stopping, reduce_lr, checkpoint, WandbCallback(input_type=\"image\", output_type=\"segmentation_mask\", log_weights=True, save_model=True)],\n",
        "#         # callbacks=[early_stopping, reduce_lr, checkpoint],\n",
        "#         use_multiprocessing=True,\n",
        "#         workers=90)\n",
        "# wandb.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6zivNBIEylj"
      },
      "source": [
        "## Results\n",
        "**256x256:**  \n",
        "Unet mask -> remove small clusters under 20 = ~50 score  \n",
        "Unet mask with 0s between touching emojis -> remove small clusters under 20 = ~41  \n",
        "Unet remove all intersecting emoji areas -> remove small clusters under 50 = ~49 iou 0.86\n",
        "\n",
        "**512x512:**  \n",
        "Unet mask with 0s between touching emojis -> remove small clusters under 100 = ~240 iou 0.93\n",
        "\n",
        "**128x128:**  \n",
        "Unet mask with 0s between touching emojis -> remove small clusters under 20 = ~212 iou 0.81"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aHHy7zYEylk"
      },
      "outputs": [],
      "source": [
        "def centroids_from_mask(pred_mask):\n",
        "    # Create regions from the segmentation mask\n",
        "    ret_coords = []\n",
        "    segments = [region for region in regionprops(label(pred_mask)) if region.area > 45]\n",
        "    \n",
        "    # Good stats\n",
        "    mean_area = np.mean([region.area for region in segments])\n",
        "    median_area = np.median([region.area for region in segments])\n",
        "    mean_perimeter = np.mean([region.perimeter for region in segments])\n",
        "    median_perimeter = np.median([region.perimeter for region in segments])\n",
        "    std_area = np.std([region.area for region in segments])\n",
        "    std_perimeter = np.std([region.perimeter for region in segments])\n",
        "    \n",
        "    # Find possible cluster elements that are otherwise too small\n",
        "    possible_clusters = [region for region in segments if region.area < median_area]\n",
        "    for c in possible_clusters:\n",
        "        segments.remove(c)\n",
        "\n",
        "    # Find possible touching regions that should be seperate (usually they're just too big)\n",
        "    possible_touching = [region for region in segments if region.area > mean_area + std_area*1.5 or region.perimeter > median_perimeter + (std_perimeter*1.5)]\n",
        "    for c in possible_touching:\n",
        "        segments.remove(c)\n",
        "\n",
        "    confirmed_centroids = [props.centroid for props in segments]\n",
        "\n",
        "    # Merge nearby clusters\n",
        "    while len(possible_clusters) > 0:\n",
        "        region = possible_clusters[0]\n",
        "        nearby_clusters = [region2 for region2 in possible_clusters if np.linalg.norm(np.array(region.centroid) - np.array(region2.centroid)) < 20]\n",
        "        if len(nearby_clusters) > 0:\n",
        "            # Merge nearby clusters\n",
        "            new_centroid = np.mean([region.centroid for region in nearby_clusters], axis=0)\n",
        "            confirmed_centroids.append(new_centroid)\n",
        "            for cluster in nearby_clusters:\n",
        "                possible_clusters.remove(cluster)\n",
        "        else:\n",
        "            # Nothing nearby, so it's a new cluster\n",
        "            confirmed_centroids.append(region.centroid)\n",
        "            possible_clusters.remove(region)\n",
        "\n",
        "    # For each large cluster, seperate it into two clusters if it is not approximately a square (which would indicate its a large emoji)\n",
        "    while len(possible_touching) > 0:\n",
        "        region = possible_touching[0]\n",
        "\n",
        "        edge_length = region.perimeter / 4\n",
        "        if edge_length * 0.9 < np.sqrt(region.area_filled) < edge_length * 1.1:\n",
        "            confirmed_centroids.append(region.centroid)\n",
        "        else:\n",
        "            cluster_centroids = KMeans(n_clusters=2, n_init='auto').fit(region.coords).cluster_centers_\n",
        "            for centroid in cluster_centroids:\n",
        "                confirmed_centroids.append(centroid)\n",
        "\n",
        "        possible_touching.remove(region)\n",
        "\n",
        "\n",
        "    ret_coords = [(int(cx*downsample_f), int(cy*downsample_f)) for cx, cy in confirmed_centroids] \n",
        "    return ret_coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtjLTpI0Eyln"
      },
      "source": [
        "## Some Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_AhHj61kEylo",
        "outputId": "addf4e26-8c55-4cb6-9a58-58d8f40174f4"
      },
      "outputs": [],
      "source": [
        "emoji_hunt_object = EmojiHunt() \n",
        "samples = [val_ds.emoji_hunt_object.generate_image_and_points() for _ in range(batch_size)]\n",
        "\n",
        "sample_imgs = [sample[0] for sample in samples]\n",
        "sample_emojis = [sample[1] for sample in samples]\n",
        "sample_ground_truth = [sample[2] for sample in samples]\n",
        "sample_emoji_mask = [sample[3] for sample in samples]\n",
        "\n",
        "inputs = np.empty((batch_size, *input_shape))\n",
        "for i, image in enumerate(sample_imgs):\n",
        "    image = cv2.resize(image, (img_width, img_height), interpolation=cv2.INTER_AREA)\n",
        "    sample_imgs[i] = image # For displaying in human-viewable form\n",
        "    val_ds.datagen.standardize(image)\n",
        "    inputs[i] = image\n",
        "\n",
        "preds = model.predict(inputs, batch_size=batch_size)\n",
        "pred_masks = np.argmax(preds['segmentation'], axis=-1)\n",
        "\n",
        "# Plot 4 results, print the image and mask seperately\n",
        "fig, axs = plt.subplots(4, 2, figsize=(12, 24))\n",
        "i = 0\n",
        "for img, mask, ground_truth, gt_mask in zip(sample_imgs, pred_masks, sample_ground_truth, sample_emoji_mask):\n",
        "    preds = centroids_from_mask(mask)\n",
        "    score = emoji_hunt_object.score_function(preds, ground_truth)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    if score > 0:\n",
        "        if i == 4:\n",
        "            break\n",
        "        \n",
        "        ax = axs[i, 0]\n",
        "        ax.set_title(f\"Score: {score}\")\n",
        "        ax.imshow(img)\n",
        "        ax.imshow(mask, alpha=0.5)\n",
        "        prev_pred = (0, 0)\n",
        "        for y, x in preds:\n",
        "            if (y, x) == prev_pred:\n",
        "                ax.add_patch(plt.Circle((x//downsample_f, y//downsample_f), 2, color='r', fill=True))\n",
        "            else:\n",
        "                ax.add_patch(plt.Circle((x//downsample_f, y//downsample_f), 1, color='r', fill=True))\n",
        "            prev_pred = (y, x)\n",
        "        ax = axs[i, 1]\n",
        "        ax.imshow(gt_mask)\n",
        "        for y, x in sample_ground_truth[i]:\n",
        "            ax.add_patch(plt.Circle((x, y), 1, color='g', fill=True))\n",
        "        i += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0A0cZxpEylr"
      },
      "source": [
        "## Score the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueji6_aCEylr",
        "outputId": "5f73f892-2b6c-40cf-a3ca-650e9d063426"
      },
      "outputs": [],
      "source": [
        "def test_good_function(image, _):\n",
        "    image = cv2.resize(image, (img_width, img_height), interpolation=cv2.INTER_AREA)\n",
        "    val_ds.datagen.standardize(image)\n",
        "    pred = model.predict(np.array([image]), batch_size=1, verbose=0)\n",
        "    pred_mask = np.argmax(pred['segmentation'], axis=-1).squeeze()\n",
        "\n",
        "    # Cluster regions of 1s\n",
        "    ret_coords = centroids_from_mask(pred_mask)\n",
        "\n",
        "    return ret_coords\n",
        "\n",
        "emoji_hunt_object = EmojiHunt() \n",
        "config = emoji_hunt_object.get_config()\n",
        "# config['emoji_transforms']['CoarseDropout'] = False\n",
        "# config['emoji_transforms']['ChangeColorTemperature'] = False\n",
        "# config['emoji_transforms']['PiecewiseAffine'] = False\n",
        "emoji_hunt_object.update_config(config)\n",
        "emoji_hunt_object.offical_test(test_good_function,emoji_hunt_object.get_config())\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2fa7c0d71b8df10136823801c054c05481f0c73beba2fe0acaf89c65f16d7f0c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
